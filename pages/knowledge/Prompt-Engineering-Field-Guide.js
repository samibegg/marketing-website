// pages/prompt-comparison.js
import React, { useState, useEffect, useRef, useCallback } from 'react';
import Head from 'next/head';
import Image from 'next/image'; // Keep even if not used directly yet
import Link from 'next/link';
import { useRouter } from 'next/router';
import { NextSeo } from 'next-seo';
import { useSession } from 'next-auth/react';
import { ArrowRight, ArrowLeft } from 'lucide-react'; // Assuming lucide-react is installed

import Header from '../../components/Header'; // Adjust path if needed
import Footer from '../../components/Footer'; // Adjust path if needed
import { useAuthModal } from '@/context/AuthModalContext'; // Adjust path if needed

/**
 * Simple debounce function.
 */
const debounce = (func, delay) => {
  let timeoutId;
  return function (...args) {
    clearTimeout(timeoutId);
    timeoutId = setTimeout(() => {
      func.apply(this, args);
    }, delay);
  };
};

/**
 * Analytics Tracking Function (Placeholder)
 */
const trackEvent = (eventName, eventData) => {
  console.log('Analytics Event:', eventName, eventData);
  // Ensure gtag is available before calling
  if (typeof window !== 'undefined' && window.gtag) {
    window.gtag('event', eventName, eventData);
  } else {
    console.warn('window.gtag not available for tracking event:', eventName);
  }
};


// --- Page Component ---
export default function PromptEngineeringFieldGuide() {
  // --- State ---
  const [openAccordion, setOpenAccordion] = useState(null); // State for accordion

  // --- Refs for Analytics ---
  const contentRef = useRef(null); // Ref for copy tracking
  const scrollMilestonesReached = useRef(new Set()); // Ref for scroll tracking
  const activeTimeStart = useRef(null); // Ref for active time tracking start
  const totalActiveTime = useRef(0); // Ref for cumulative active time

  // --- Auth and Navigation Hooks ---
  const { data: session, status } = useSession(); // Get session status
  const { openModal } = useAuthModal(); // Get function to open login modal
  const router = useRouter(); // Get router instance

  // --- Accordion Toggle ---
  const toggleAccordion = (key) => {
    setOpenAccordion(openAccordion === key ? null : key);
    trackEvent('accordion_toggle', { section_id: key, is_open: openAccordion !== key });
  };

  // --- Conditional Button Logic ---
  const nextPageRoute = '/knowledge/prompt-engineering-tailoring-for-llms'; // Define the target page

  const handleConditionalClick = () => {
    const action = status === 'authenticated' ? 'navigate_next_section' : 'login_prompted';
    trackEvent('cta_click', { button_text: buttonText, action: action });

    if (status === 'authenticated') {
      router.push(nextPageRoute);
    } else if (status === 'unauthenticated') {
      openModal();
    }
    // Do nothing if status is 'loading'
  };

  // Determine button text based on auth status
  let buttonText = 'Loading...';
  if (status === 'authenticated') {
    buttonText = 'Go to Next Section';
  } else if (status === 'unauthenticated') {
    buttonText = 'Login to Continue';
  }
  // --- End Conditional Button Logic ---


  // --- Analytics Tracking Effects ---

  // 1. Track initial page view
  useEffect(() => {
    trackEvent('page_view', { page_title: 'Prompt Engineering Field Guide' });
  }, []); // Empty dependency array ensures this runs only once on mount

  // 2. Track Scroll Depth
  useEffect(() => {
    const handleScroll = () => {
      const scrollableHeight = document.documentElement.scrollHeight - window.innerHeight;
      if (scrollableHeight <= 0) return; // Avoid division by zero

      const scrollPercent = Math.min(100, Math.round((window.scrollY / scrollableHeight) * 100));
      const milestones = [25, 50, 75, 100];

      milestones.forEach(milestone => {
        if (scrollPercent >= milestone && !scrollMilestonesReached.current.has(milestone)) {
          scrollMilestonesReached.current.add(milestone);
          // Consider making page_section dynamic based on visible content if needed
          trackEvent('scroll_depth', { depth_percentage: milestone, page_section: 'Prompt Techniques Overview' });
        }
      });
    };

    const debouncedScrollHandler = debounce(handleScroll, 250); // Debounce scroll events
    window.addEventListener('scroll', debouncedScrollHandler);

    // Cleanup function to remove the event listener
    return () => window.removeEventListener('scroll', debouncedScrollHandler);
  }, []); // Empty dependency array

  // 3. Track Active Time Spent
  useEffect(() => {
    const handleVisibilityChange = () => {
      if (document.visibilityState === 'hidden') {
        // If tab becomes hidden, add the time elapsed since it became active
        if (activeTimeStart.current) {
             totalActiveTime.current += Date.now() - activeTimeStart.current;
             activeTimeStart.current = null; // Reset start time
        }
      } else if (document.visibilityState === 'visible') {
        // If tab becomes visible, record the start time
        activeTimeStart.current = Date.now();
      }
    };

    const handleBeforeUnload = () => {
       // If the page is visible when unloading, add the final time segment
      if (document.visibilityState === 'visible' && activeTimeStart.current) {
        totalActiveTime.current += Date.now() - activeTimeStart.current;
      }
      // Send the final event only if some time was actually spent
      if (totalActiveTime.current > 1000) { // Send only if > 1 second
        trackEvent('active_time_spent', {
            duration_seconds: Math.round(totalActiveTime.current / 1000),
            page_section: 'Prompt Techniques Overview' // Adjust section if needed
        });
      }
    };

    // Set initial start time
    if (document.visibilityState === 'visible') {
        activeTimeStart.current = Date.now();
    }

    document.addEventListener('visibilitychange', handleVisibilityChange);
    window.addEventListener('beforeunload', handleBeforeUnload); // Track time when closing tab/browser

    // Cleanup function
    return () => {
      document.removeEventListener('visibilitychange', handleVisibilityChange);
      window.removeEventListener('beforeunload', handleBeforeUnload);
      // Send final time if component unmounts while visible (e.g., navigation)
      handleBeforeUnload();
    };
  }, []); // Empty dependency array

  // 4. Track Text Copying
  useEffect(() => {
    const handleCopy = (event) => {
      const selectedText = window.getSelection()?.toString() || '';
      if (selectedText.length > 10) { // Track only if a meaningful amount is copied
        trackEvent('text_copied', {
          page_section: 'Prompt Techniques Overview', // Make dynamic if possible
          content_snippet: selectedText.substring(0, 100) + (selectedText.length > 100 ? '...' : '')
        });
      }
    };

    // Attach listener to the main content area using the ref
    const contentElement = contentRef.current;
    if (contentElement) {
        contentElement.addEventListener('copy', handleCopy);
    }

    // Cleanup function
    return () => {
      if (contentElement) {
        contentElement.removeEventListener('copy', handleCopy);
      }
    };
  }, []); // Empty dependency array
  // --- End Analytics Tracking Effects ---


  // --- SEO Content ---
  const pageTitle = "Prompt Engineering Field Guide: Techniques Compared"; // Make title more specific
  const pageDescription = "A comparative overview and field guide to essential prompt engineering techniques for LLMs, from zero-shot to ReAct, ToT, and GoT."; // Expanded description
  const canonicalUrl = "https://www.forgemission.com/knowledge/prompt-engineering-field-guide"; // Ensure this is the correct final URL
  const imageUrl = "https://www.forgemission.com/images/og-prompt-engineering-guide.jpg"; // Use a specific OG image if possible
  const publicationDate = "2025-05-05T10:00:00Z"; // Adjust date
  const modifiedDate = "2025-05-06T11:30:00Z"; // Add modified date if applicable

  // --- JSON-LD Structured Data (Improved) ---
  const jsonLd = {
    "@context": "https://schema.org",
    "@type": "TechArticle", // More specific type
    "headline": pageTitle,
    "description": pageDescription,
    "image": imageUrl,
    "datePublished": publicationDate,
    "dateModified": modifiedDate, // Add modified date
    "author": {
        "@type": "Organization",
        "name": "ForgeMission", // Replace if needed
        "url": "https://www.forgemission.com" // Add organization URL
    },
    "publisher": {
        "@type": "Organization", // Use Organization type consistently
        "name": "ForgeMission", // Replace if needed
        "logo": {
            "@type": "ImageObject",
            "url": "https://www.forgemission.com/images/logo.png" // Replace if needed
        }
    },
    "mainEntityOfPage": {
        "@type": "WebPage",
        "@id": canonicalUrl
    },
    "keywords": "prompt engineering, large language models, llm, zero-shot, few-shot, chain-of-thought, react prompting, tree of thoughts, graph of thoughts, ai prompting techniques" // Add relevant keywords
  };
  // --- End SEO Content ---


  return (
    <>
      {/* --- SEO Components --- */}
      <NextSeo
        title={pageTitle}
        description={pageDescription}
        canonical={canonicalUrl}
        openGraph={{
          type: 'article', // Use 'article' for blog posts/guides
          url: canonicalUrl,
          title: pageTitle,
          description: pageDescription,
          images: [
            {
              url: imageUrl, // Ensure this image is optimized for OG (e.g., 1200x630)
              width: 1200, // Specify dimensions if known
              height: 630,
              alt: pageTitle,
            },
          ],
          article: { // Add article specific OG tags
            publishedTime: publicationDate,
            modifiedTime: modifiedDate,
            // section: 'Artificial Intelligence', // Optional section
            authors: [
              'https://www.forgemission.com', // Link to author profile or org page
            ],
            tags: ['Prompt Engineering', 'LLM', 'AI Strategy', 'Artificial Intelligence'], // Add relevant tags
          },
        }}
        twitter={{
          cardType: 'summary_large_image',
          // site: '@YourTwitterHandle', // Optional: Add Twitter handle
          handle: '@YourTwitterHandle', // Optional: Add Twitter handle
          // title, description, image are often inherited from openGraph, but can be specified
        }}
      />
      <Head>
        {/* Standard Head elements */}
        <meta charSet="utf-8" />
        <meta name="viewport" content="width=device-width, initial-scale=1" />
        {/* Add JSON-LD */}
        <script
          type="application/ld+json"
          dangerouslySetInnerHTML={{ __html: JSON.stringify(jsonLd) }}
          key="article-jsonld" // Add a key for React reconciliation
        />
        {/* Add other meta tags if needed (e.g., theme-color) */}
      </Head>
      {/* --- End SEO Components --- */}


      <div className="min-h-screen flex flex-col bg-gray-50 dark:bg-gray-900">
        <Header /> {/* Ensure Header/Footer support dark mode */}

        {/* Use the ref here for copy tracking */}
        <main ref={contentRef} className="flex-grow px-4 sm:px-6 py-12 max-w-4xl mx-auto w-full">
          {/* Back Link */}
          <div className="mb-8">
            <Link href="/knowledge-base" className="inline-flex items-center text-sm font-medium text-blue-600 dark:text-blue-400 hover:text-blue-800 dark:hover:text-blue-300 transition">
              <ArrowLeft className="w-4 h-4 mr-2" />
              Back to Knowledge Base
            </Link>
          </div>

          {/* --- Intro Section 1 --- */}
          <div className="bg-white dark:bg-gray-800 rounded-2xl shadow-lg p-6 sm:p-8 mb-12 animate-fadeIn border border-gray-200 dark:border-gray-700">
            <div className="flex items-center mb-4">
              <span className="text-2xl mr-2" aria-hidden="true">üìå</span>
              <span className="text-xs bg-blue-100 dark:bg-blue-900 text-blue-800 dark:text-blue-200 font-semibold px-3 py-1 rounded-full uppercase tracking-wider">
                Insight
              </span>
            </div>
            <h1 className="text-3xl font-bold mb-4 text-gray-900 dark:text-gray-100">
              Prompt Engineering: The New Power Skill No One Told You About
            </h1>
            <p className="mb-4 text-gray-700 dark:text-gray-300 leading-relaxed">
              You‚Äôve heard the buzzwords‚ÄîChatGPT, generative AI, large language models. Maybe you‚Äôve even seen some slick demos. But here‚Äôs what most of the hype skips over: the real magic doesn‚Äôt come from the model‚Äîit comes from <strong>the prompt</strong>.
            </p>
            <p className="mb-4 text-gray-700 dark:text-gray-300 leading-relaxed">
              That‚Äôs right. In this new AI era, it‚Äôs not just what the machine can do, it‚Äôs how well <em>you</em> can ask it to do it. This is the quiet revolution happening behind the scenes: <strong>prompt engineering</strong>, a deceptively simple yet profoundly strategic practice that‚Äôs starting to separate AI dabblers from those actually driving results.
            </p>
            <p className="mb-4 text-gray-700 dark:text-gray-300 leading-relaxed">
              We're not talking about tweaking code or training models. We‚Äôre talking about crafting precise, structured language that coaxes the best out of probabilistic systems‚Äîsystems that think in tokens and patterns, not logic and loops. This is less like programming and more like negotiating with a highly trained expert who speaks fluent probability and metaphor. Get it right, and you can summarize complex documents, generate compelling marketing copy, debug code, or even simulate strategic decisions. Get it wrong, and you'll be drowning in plausible nonsense.
            </p>
            <p className="mb-4 text-gray-700 dark:text-gray-300 leading-relaxed">
              Sound abstract? It is‚Äîuntil you understand how it really works.
            </p>
            <p className="text-gray-700 dark:text-gray-300 font-medium">
              In this post, we‚Äôll pull back the curtain on what prompt engineering actually involves, why it matters, and how business and technical leaders alike can start building intuition around this essential skill. If you're serious about leveraging AI beyond the surface-level gimmicks, keep reading.
            </p>
          </div>
          {/* --- End Intro Section 1 --- */}


          {/* --- Intro Section 2 --- */}
          <div className="bg-gray-50 dark:bg-gray-800/50 rounded-2xl shadow-inner p-6 sm:p-8 mb-12 border border-gray-200 dark:border-gray-700">
            <h2 className="text-2xl sm:text-3xl font-extrabold text-gray-900 dark:text-gray-100 mb-4">
              Think You Know Prompts? Think Again.
            </h2>
            <p className="mb-4 text-gray-700 dark:text-gray-300 leading-relaxed">
              If you believe prompt engineering is just about asking better questions, this report will change your mind‚Äîand maybe your roadmap.
            </p>
            <p className="mb-4 text-gray-700 dark:text-gray-300 leading-relaxed">
              Behind every impressive AI demo lies a hidden layer of strategy: <strong>the prompt</strong>. And while most conversations stop at ‚Äújust tell the AI what you want,‚Äù the truth is far more nuanced‚Äîand far more powerful. This isn‚Äôt just about clever phrasing. It‚Äôs about mastering a new interface between human intention and machine behavior.
            </p>
            <p className="mb-4 text-gray-700 dark:text-gray-300 leading-relaxed">
              In this report, we break down the full landscape of prompt engineering for large language models (LLMs), from foundational tricks to bleeding-edge reasoning frameworks. We‚Äôll start with the basics‚Äî<strong>zero-shot</strong>, <strong>few-shot</strong>, <strong>role prompting</strong>‚Äîand quickly move into the sophisticated methods that drive real performance: <strong>Chain-of-Thought</strong>, <strong>ReAct</strong>, <strong>Tree of Thoughts</strong>, <strong>Graph of Thoughts</strong>, and more.
            </p>
            <p className="mb-4 text-gray-700 dark:text-gray-300 leading-relaxed">
              You'll get a side-by-side comparison of techniques based on effectiveness, complexity, and cost, along with practical guidance on tailoring prompts for different LLM types‚Äîwhether you're working with <strong>GPT-4o</strong>, <strong>Claude 3</strong>, <strong>Llama 3</strong>, <strong>Gemini</strong>, or <strong>Mistral</strong>. We also cover how factors like model size, tuning style, and architecture affect what works‚Äîand what fails.
            </p>
            <p className="mb-4 text-gray-700 dark:text-gray-300 leading-relaxed">
              From <strong>multilingual prompts</strong> to <strong>managing ambiguity</strong>, from <strong>prompt length</strong> to <strong>iterative refinement</strong>, from <strong>implementation patterns</strong> to <strong>performance evaluation metrics</strong>, this report goes deep. And we don‚Äôt shy away from the tough stuff either: ethical dilemmas, emerging risks, and the foggy future of prompt-native systems.
            </p>
            <p className="text-gray-700 dark:text-gray-300 font-medium">
              If you're building with LLMs‚Äîor planning to‚Äîyou‚Äôll want this knowledge in your toolkit. <strong>This is your field guide to prompt engineering:</strong> strategic, technical, and designed to separate serious practitioners from weekend tinkerers.
            </p>
          </div>
           {/* --- End Intro Section 2 --- */}


          {/* --- Accordion Section --- */}
          <div className="space-y-4 mb-12">
            {[
              { key: "zero", title: "Zero-Shot Prompting: The No-Frills, High-Stakes First Move", content: (<> <p> Zero-shot prompting is the LLM equivalent of throwing it in the deep end and seeing if it swims. No examples, no training wheels‚Äîjust a clear instruction and the model‚Äôs raw, pre-trained intelligence to get the job done. </p> <p> <strong>Think:</strong><br /> ‚ÄúTranslate this sentence to French: Hello, world!‚Äù<br /> ‚ÄúSummarize this article: [insert text here]‚Äù </p> <p> You‚Äôre not teaching the model how to do the task‚Äîyou‚Äôre trusting that it‚Äôs already seen enough to figure it out. And with today‚Äôs top-tier models (think GPT-4o, Claude 3), that‚Äôs often a pretty safe bet‚Äîfor the right kind of tasks. </p> <p> <strong>Why it works:</strong> It‚Äôs dead simple. Fast. Cheap. There‚Äôs no setup, no example crafting, no extra tokens bloating your cost. For tasks that fall squarely within what the model was trained on‚Äîsummarization, basic Q&A, straightforward text generation‚Äîit often just works. Instruction tuning and RLHF have only made this better, turning LLMs into surprisingly capable zero-shot performers. </p> <p> <strong>But here‚Äôs the catch:</strong> Without examples, you're flying without a net. The model might misunderstand your intent, ignore your formatting request, or just hallucinate something confidently wrong. For complex reasoning or highly structured outputs, performance can drop fast. You‚Äôre not in the driver‚Äôs seat‚Äîyou‚Äôre more like a passenger giving verbal directions to a very polite but unpredictable assistant. </p> <div> <strong>When to use it:</strong> <ul className="list-disc pl-6 mt-2 space-y-1"> <li>General knowledge lookups</li> <li>Basic content generation</li> <li>Quick-and-dirty summaries</li> <li>Simple classification tasks</li> </ul> </div> <p> In short: zero-shot is your go-to for speed and simplicity‚Äîbut don‚Äôt mistake it for precision engineering. It‚Äôs a powerful tool if you know when to use it‚Äîand when to bring in something smarter. </p> </>) },
              { key: "few", title: "Few-Shot Prompting: Show, Don‚Äôt Just Tell", content: (<> <p> If zero-shot is a blind trust fall, few-shot prompting gives the model a hint‚Äîor three. Instead of just issuing a command, you show the model what you want by including a few carefully chosen examples in your prompt. Think of it as setting the vibe before asking for a performance. </p> <p> <strong>Example:</strong><br /> ‚ÄúTranslate to French:<br /> Hello, world! ‚Üí Bonjour le monde!<br /> How are you? ‚Üí Comment √ßa va?<br /> Good night ‚Üí [??]‚Äù </p> <p> Now the model isn‚Äôt guessing your intent‚Äîit‚Äôs pattern matching. You‚Äôre priming it with structure, tone, and style, so it knows not just what to do, but <em>how</em> to do it. </p> <p> <strong>Why it works:</strong> LLMs are masters of pattern recognition. A few well-placed examples can dramatically boost performance, especially for tasks that aren‚Äôt obvious from a standalone instruction. Few-shot prompting makes things like tone matching, formatting, or domain-specific nuance easier to lock in‚Äîwithout modifying the model or writing a line of code. </p> <p> <strong>What to watch out for:</strong> Those extra examples don‚Äôt come free. Each one eats up precious tokens‚Äîespecially problematic for longer inputs or smaller context windows. And if your examples are inconsistent, unclear, or irrelevant, they‚Äôll just confuse the model. Garbage in, garbage out. </p> <div> <strong>Best for:</strong> <ul className="list-disc pl-6 mt-2 space-y-1"> <li>Custom tone/style generation</li> <li>Structured outputs with specific formatting</li> <li>Domain-specific tasks (e.g., legal text classification)</li> <li>Bootstrapping behavior when zero-shot falls flat</li> </ul> </div> <p> Few-shot prompting is like giving your AI intern a few sample reports before asking for the next one. If you care about quality, it‚Äôs often well worth the investment. </p> </>) },
              { key: "role", title: "Role Prompting: Cast the LLM, Then Let It Perform", content: (<> <p> Sometimes, the fastest way to get the right output is to assign the model a role‚Äîliterally. Role prompting primes the LLM by telling it who it is supposed to be before asking it to perform a task. </p> <p> <strong>Example:</strong><br /> ‚ÄúYou are a world-class cybersecurity analyst. Given the following log file, identify any signs of a breach.‚Äù<br /> ‚ÄúAct as a startup pitch coach. Rewrite this elevator pitch to appeal to investors.‚Äù </p> <p> <strong>Why it works:</strong> Large language models are trained on oceans of role-specific content‚Äîemails from lawyers, advice from therapists, answers from teachers, feedback from editors. When you set a role, you immediately narrow the model‚Äôs behavioral space, activating relevant tone, style, and domain-specific reasoning patterns. It‚Äôs a subtle way to raise the floor and the ceiling of performance‚Äîno extra logic or training required. </p> <p> <strong>What to watch out for:</strong> Roles work best when they‚Äôre believable, specific, and aligned with the task. Go too vague (‚Äúact professional‚Äù) or overly abstract, and the model may flounder. Also, stacking too many roles can cause confusion or dilute the effect. </p> <div> <strong>Best for:</strong> <ul className="list-disc pl-6 mt-2 space-y-1"> <li>Domain adaptation (e.g., legal, medical, marketing)</li> <li>Consistent tone/style generation</li> <li>Guiding the model toward best practices or frameworks</li> <li>Teaching the model how to frame an answer before generating it</li> </ul> </div> <p> Role prompting doesn‚Äôt just ask the model to perform‚Äîit casts it in the right mindset. Treat it like method acting for LLMs, and you‚Äôll get better results with less effort. </p> </>) },
              { key: "structured", title: "Structured Prompting: Set the Format, Shape the Output", content: (<> <p> Want consistent, clean, parseable results from your LLM? Then you need structured prompting‚Äîthe art of specifying the format before the model starts talking. </p> <p> <strong>Example:</strong><br /> ‚ÄúRespond in the following JSON format:<br /> {'{ "summary": "...", "sentiment": "positive/neutral/negative", "keywords": ["..."] }'}<br /> ‚ÄúProvide your answer in bullet points, grouped under ‚ÄòPros‚Äô, ‚ÄòCons‚Äô, and ‚ÄòNext Steps‚Äô.‚Äù </p> <p> <strong>Why it works:</strong> LLMs are flexible‚Äîbut without structure, they tend to improvise. And that‚Äôs fine for storytelling, but terrible for downstream use, automation, or analysis. Structured prompting constrains the output shape, helping with reliability, consistency, and machine-readability. It‚Äôs essential for integrating LLMs into real-world workflows and pipelines. </p> <p> <strong>What to watch out for:</strong> The model isn‚Äôt a strict parser‚Äîit follows structure approximately. Complex schemas may still require post-processing or validation. Also, overly rigid prompts can sometimes reduce creativity or cause the model to freeze if it‚Äôs unsure how to fill a field. </p> <div> <strong>Best for:</strong> <ul className="list-disc pl-6 mt-2 space-y-1"> <li>API-ready output (e.g., JSON, tables, YAML)</li> <li>Business reports, summaries, templates</li> <li>Workflow integration (e.g., Zapier, LangChain, custom apps)</li> <li>Making LLMs play nicely with code and other tools</li> </ul> </div> <p> Structured prompting is the difference between ‚Äúchatbot‚Äù and toolchain component. If you want your AI to fit into a system, not just give an answer, structure is non-negotiable. </p> </>) },
              { key: "cot", title: "Chain-of-Thought Prompting: Make It Think, Step by Step", content: (<> <p> Want your LLM to reason instead of guess? Don‚Äôt just ask for the answer‚Äîask it to think. That‚Äôs the core idea behind Chain-of-Thought (CoT) prompting: instead of jumping straight to the final output, you guide the model to show its work‚Äîjust like a student solving a math problem. </p> <p> <strong>Example:</strong><br /> ‚ÄúQ: If a train leaves at 2 PM going 60 mph and another at 3 PM going 90 mph, when will they meet?<br /> A: Let‚Äôs think step by step‚Ä¶‚Äù </p> <p> <strong>Why it works:</strong> LLMs aren‚Äôt calculators‚Äîthey‚Äôre token predictors. But when prompted to generate intermediate steps, they‚Äôre better able to avoid logic gaps and hallucinations. CoT gives you a window into the model‚Äôs ‚Äúthinking process‚Äù and often leads to more reliable conclusions. </p> <p> <strong>What to watch out for:</strong> CoT adds length‚Äîboth in prompt and output. That means more tokens, higher cost, and potential context limits. And if the model gets off track mid-thought? The final answer will still sound confident‚Ä¶ and still be wrong. </p> <div> <strong>Best for:</strong> <ul className="list-disc pl-6 mt-2 space-y-1"> <li>Math and logic problems</li> <li>Multi-step reasoning tasks</li> <li>Any complex decision-making that requires traceability</li> <li>Situations where explainability matters (audits, compliance, etc.)</li> </ul> </div> <p> CoT is your prompt‚Äôs version of slow is smooth, smooth is fast. It‚Äôs not just about getting the answer‚Äîit‚Äôs about why the answer makes sense. And in business, that‚Äôs often what separates automation from actual intelligence. </p> </>) },
              { key: "selfConsistency", title: "Self-Consistency: Ask Again, Smarter", content: (<> <p> Chain-of-Thought helps models reason. But what if they reason differently every time? That‚Äôs where Self-Consistency comes in. It embraces the probabilistic nature of LLMs‚Äîrather than fearing variability, it uses it as a strength. </p> <p> <strong>How it works:</strong> You run the same CoT prompt multiple times, let the model generate several different reasoning paths, and then aggregate the answers‚Äîusually by voting or frequency. It‚Äôs like getting five smart consultants in a room, letting them think independently, and then going with the most common solution. </p> <p> <strong>Why it works:</strong> LLMs often produce better answers when ‚Äúthinking out loud‚Äù‚Äîbut they don‚Äôt always arrive at the same conclusion. By generating multiple diverse outputs and filtering for consensus, you reduce the risk of flukes and hallucinations. It‚Äôs statistical grounding meets structured thinking. </p> <p> <strong>What to watch out for:</strong> Self-Consistency isn‚Äôt fast. You‚Äôre paying for multiple completions and then post-processing the result. It‚Äôs ideal when accuracy is worth the wait‚Äîbut overkill for quick-turn tasks or low-stakes outputs. </p> <div> <strong>Best for:</strong> <ul className="list-disc pl-6 mt-2 space-y-1"> <li>High-stakes decision support</li> <li>Complex reasoning (math, analysis, structured logic)</li> <li>Scenarios where reliability &gt speed</li> <li>Building confidence in model responses</li> </ul> </div> <p> If CoT makes your LLM think, Self-Consistency makes it double-check itself‚Äîa powerful combination when precision matters. </p> </>) },
              { key: "gkp", title: "Generated Knowledge Prompting: Teach the Model Before You Ask", content: (<> <p> What if your LLM doesn‚Äôt quite have the answer‚Äîbut could find it, if it just knew a little more? Enter Generated Knowledge Prompting (GKP): a strategy where you first have the model generate relevant background information, and then use that as part of the actual prompt to complete the task. </p> <p> <strong>Example flow:</strong><br /> Prompt: ‚ÄúExplain what the concept of zero trust architecture means in cybersecurity.‚Äù<br /> Use that explanation as part of the context for a follow-up task:<br /> ‚ÄúNow, using that background, assess the strengths and weaknesses of a zero trust implementation in a financial services environment.‚Äù </p> <p> <strong>Why it works:</strong> Even the best LLMs can flounder if the original prompt assumes too much. By explicitly generating foundational knowledge first‚Äîeither manually or through prompt chaining‚Äîyou reduce the cognitive load on the model during the main task. This is especially powerful when tackling niche, emerging, or complex domains that the model wasn‚Äôt deeply trained on. </p> <p> <strong>What to watch out for:</strong> Generated knowledge is only as good as the prompt that created it. Hallucinations and inaccuracies can creep in, especially when dealing with facts or sensitive domains. You‚Äôll want to include validation, retrieval augmentation, or human-in-the-loop checks when precision matters. </p> <div> <strong>Best for:</strong> <ul className="list-disc pl-6 mt-2 space-y-1"> <li>Multi-step analysis where foundational context is missing</li> <li>Domain-specific tasks (e.g., legal, technical, financial)</li> <li>Prompt chains that build reasoning layer by layer</li> <li>Teams using LLMs to scale knowledge work across unfamiliar areas</li> </ul> </div> <p> Generated Knowledge Prompting flips the script: instead of assuming the model knows, you make it learn first‚Äîon demand, in-context, and purpose-built for the job ahead. </p> </>) },
              { key: "react", title: "ReAct: Reasoning Meets Action", content: (<> <p> ReAct (short for Reasoning + Acting) turns your LLM from a passive generator into a decision-making agent that can both think through a problem and take steps toward solving it‚Äîlike searching the web, querying tools, or consulting external data sources mid-prompt. </p> <p> <strong>Example:</strong><br /> ‚ÄúLet‚Äôs think step by step.‚Äù<br /> [Reasoning‚Ä¶]<br /> ‚ÄúI should look this up.‚Äù<br /> [Searches‚Ä¶]<br /> [Continues reasoning with new info] </p> <p> <strong>Why it works:</strong> Pure reasoning is limited by what the model already knows. ReAct blends thinking with doing. It allows the model to iteratively reason, take an action, observe the result, and continue‚Äîall within one prompt loop. This mirrors how humans solve problems: research, assess, refine. </p> <p> <strong>What to watch out for:</strong> ReAct requires an agentic framework‚Äîthe model alone can‚Äôt take external actions. You need infrastructure (like LangChain, OpenAI function calling, or custom tools) to make this work. Also, ReAct can get expensive fast, since it chains multiple steps with external calls. </p> <div> <strong>Best for:</strong> <ul className="list-disc pl-6 mt-2 space-y-1"> <li>Retrieval-augmented generation (RAG)</li> <li>Real-time decision support</li> <li>Research agents, task automation</li> <li>Complex problem solving that requires fresh data or tool use</li> </ul> </div> <p> ReAct isn‚Äôt just prompt engineering‚Äîit‚Äôs system design. When you want your AI to think and act with purpose, this is your go-to strategy. </p> </>) },
              { key: "tot", title: "Tree of Thoughts (ToT): Strategic Reasoning at Scale", content: (<> <p> Tree of Thoughts (ToT) adds strategy to Chain-of-Thought reasoning. Instead of one path from question to answer, ToT generates multiple branches of reasoning, explores them in parallel, and selects the best outcome based on defined criteria. </p> <p> <strong>Why it works:</strong> Complex problems rarely have a single solution path. ToT embraces that by exploring multiple options, backtracking if needed, and using search-like algorithms to score and compare outcomes. It brings planning and evaluation into the reasoning process‚Äîsomething standard prompts can‚Äôt do. </p> <p> <strong>What to watch out for:</strong> ToT is powerful but compute-intensive. You‚Äôll need orchestration logic (again, think LangChain-style frameworks), evaluation functions, and smart stopping rules. It‚Äôs best suited for advanced use cases where brute-force prompting just isn‚Äôt enough. </p> <div> <strong>Best for:</strong> <ul className="list-disc pl-6 mt-2 space-y-1"> <li>Creative problem solving</li> <li>Strategic planning and optimization</li> <li>Multi-step logic tasks with branching outcomes</li> <li>Scenarios where exploration beats immediacy</li> </ul> </div> <p> Tree of Thoughts turns your prompt into a multi-path search tree‚Äîthe difference between a hunch and a plan. </p> </>) },
              { key: "got", title: "Graph of Thoughts (GoT): Reasoning Without Limits", content: (<> <p> Graph of Thoughts (GoT) breaks the mold of structured reasoning. Instead of a fixed tree with branches, GoT allows nodes of reasoning to connect in any direction‚Äîforward, backward, sideways. It models thinking as a flexible, dynamic graph. </p> <p> <strong>Why it works:</strong> Real-world reasoning isn‚Äôt linear. It loops, revisits, merges, diverges. GoT captures this by allowing arbitrary relationships between ‚Äúthoughts,‚Äù enabling LLMs to work more like a network of ideas than a single path. It‚Äôs ideal for tasks requiring creativity, iteration, and lateral thinking. </p> <p> <strong>What to watch out for:</strong> GoT is bleeding-edge. It requires a strong scaffolding system and careful orchestration of how thoughts (nodes) are evaluated and connected. It‚Äôs not something you run as a simple prompt‚Äîit‚Äôs a full reasoning architecture. </p> <div> <strong>Best for:</strong> <ul className="list-disc pl-6 mt-2 space-y-1"> <li>Creative generation (stories, designs, innovations)</li> <li>Complex planning with many interdependencies</li> <li>Knowledge synthesis and mapping</li> <li>Any use case that benefits from non-linear thought</li> </ul> </div> <p> Graph of Thoughts isn‚Äôt just next-gen prompting‚Äîit‚Äôs prompt-native cognition. This is where prompt engineering begins to look like actual thinking. </p> </>) },
            ].map((item) => (
              <div key={item.key} className="border border-gray-200 dark:border-gray-700 rounded-xl bg-white dark:bg-gray-800 shadow-sm overflow-hidden">
                <button
                  className="w-full text-left px-4 py-3 flex items-center justify-between hover:bg-gray-50 dark:hover:bg-gray-700 transition-colors"
                  onClick={() => toggleAccordion(item.key)}
                  aria-expanded={openAccordion === item.key}
                  aria-controls={`accordion-content-${item.key}`}
                >
                  <span className="text-base font-semibold text-gray-800 dark:text-gray-100">
                    {item.title}
                  </span>
                  <svg
                    className={`w-5 h-5 text-gray-500 dark:text-gray-400 transition-transform duration-300 ${openAccordion === item.key ? "rotate-180" : ""}`}
                    fill="currentColor"
                    viewBox="0 0 20 20"
                  >
                    <path fillRule="evenodd" d="M5.23 7.21a.75.75 0 011.06.02L10 11.293l3.71-4.06a.75.75 0 111.08 1.04l-4.25 4.66a.75.75 0 01-1.08 0L5.25 8.27a.75.75 0 01-.02-1.06z" clipRule="evenodd" />
                  </svg>
                </button>
                {openAccordion === item.key && (
                  <div
                    id={`accordion-content-${item.key}`}
                    className="px-4 pb-4 pt-2 text-sm text-gray-700 dark:text-gray-300 space-y-3 border-t border-gray-200 dark:border-gray-700"
                  >
                    {item.content}
                  </div>
                )}
              </div>
            ))}
          </div>
          {/* --- End Accordion Section --- */}


          {/* --- Comparison Table --- */}
          <h2 className="text-2xl sm:text-3xl font-extrabold text-gray-900 dark:text-gray-100 mb-6 text-center">
            Prompt Engineering Techniques: Comparative Analysis
          </h2>
          <div className="overflow-x-auto shadow-md rounded-lg mb-12">
            <table className="w-full border-collapse text-sm md:text-base bg-white dark:bg-gray-800">
              <thead className="bg-gray-100 dark:bg-gray-700">
                <tr>
                  <th className="border dark:border-gray-600 px-4 py-3 text-left font-semibold text-gray-700 dark:text-gray-200">Technique</th>
                  <th className="border dark:border-gray-600 px-4 py-3 text-left font-semibold text-gray-700 dark:text-gray-200">Effectiveness</th>
                  <th className="border dark:border-gray-600 px-4 py-3 text-left font-semibold text-gray-700 dark:text-gray-200">Complexity</th>
                  <th className="border dark:border-gray-600 px-4 py-3 text-left font-semibold text-gray-700 dark:text-gray-200">Cost</th>
                  <th className="border dark:border-gray-600 px-4 py-3 text-left font-semibold text-gray-700 dark:text-gray-200">Best Use Cases</th>
                </tr>
              </thead>
              <tbody className="divide-y divide-gray-200 dark:divide-gray-700">
                {[
                  { name: "Zero-shot", effectiveness: "‚ö™ Medium", complexity: "üü¢ Very low", cost: "üü¢ Minimal", useCases: "General tasks, prototyping, low-stakes queries" },
                  { name: "Few-shot", effectiveness: "üü° Higher for nuanced tasks", complexity: "üü° Medium", cost: "üü° Moderate", useCases: "Classification, formatting, style transfer" },
                  { name: "Role Prompting", effectiveness: "üü¢ High", complexity: "üü¢ Low", cost: "üü¢ Minimal", useCases: "Domain adaptation, tone/style control" },
                  { name: "Structured Prompting", effectiveness: "üü¢ High", complexity: "üü° Medium", cost: "üü¢ Minimal‚ÄìModerate", useCases: "APIs, workflows, system integration" },
                  { name: "Chain-of-Thought (CoT)", effectiveness: "üü¢ High", complexity: "üü° Medium", cost: "üü° Moderate", useCases: "Logic, stepwise math, complex Q&A" },
                  { name: "Self-Consistency (CoT++)", effectiveness: "üü¢ Very high", complexity: "üî¥ High", cost: "üî¥ High", useCases: "Research, complex analysis" },
                  { name: "Generated Knowledge", effectiveness: "üü¢ High", complexity: "üü° Medium‚ÄìHigh", cost: "üü° Moderate‚ÄìHigh", useCases: "Knowledge work, onboarding LLMs" },
                  { name: "ReAct", effectiveness: "üü¢ Very high", complexity: "üî¥ High", cost: "üî¥ High", useCases: "Agents, workflows, decision support" },
                  { name: "Tree of Thoughts (ToT)", effectiveness: "üü¢ Very high", complexity: "üî¥ High", cost: "üî¥ High", useCases: "Optimization, planning" },
                  { name: "Graph of Thoughts (GoT)", effectiveness: "üü¢ Experimental but promising", complexity: "üî¥ Very high", cost: "üî¥ Very high", useCases: "Creative generation, ideation" },
                ].map((item, idx) => (
                  <tr key={idx} className="hover:bg-gray-50 dark:hover:bg-gray-700/50">
                    <td className="border dark:border-gray-600 px-4 py-3 font-medium text-gray-900 dark:text-gray-100">{item.name}</td>
                    <td className="border dark:border-gray-600 px-4 py-3 text-gray-700 dark:text-gray-300">{item.effectiveness}</td>
                    <td className="border dark:border-gray-600 px-4 py-3 text-gray-700 dark:text-gray-300">{item.complexity}</td>
                    <td className="border dark:border-gray-600 px-4 py-3 text-gray-700 dark:text-gray-300">{item.cost}</td>
                    <td className="border dark:border-gray-600 px-4 py-3 text-gray-700 dark:text-gray-300">{item.useCases}</td>
                  </tr>
                ))}
              </tbody>
            </table>
          </div>
          <div className="mb-12 text-gray-600 dark:text-gray-400 text-xs text-center">
            <p>üü¢ = Advantage &nbsp; üü° = Trade-off &nbsp; üî¥ = Cost/Complexity Flag</p>
          </div>
          {/* --- End Comparison Table --- */}


          {/* --- Call to Action / Next Section Prompt --- */}
          <div className="mt-12 pt-6 flex justify-center items-center text-center">
            <p className="text-lg font-semibold text-gray-800 dark:text-gray-200">
              Want to go deeper? Continue for implementation strategies and real-world examples.
            </p>
          </div>
          {/* --- End Call to Action --- */}


          {/* --- Navigation Buttons Container --- */}
          <div className="mt-8 pt-6 border-t border-gray-200 dark:border-gray-700 flex justify-center items-center">
            <div>
              {/* Conditional Next/Login Button */}
              <button
                onClick={handleConditionalClick}
                disabled={status === 'loading'}
                className="inline-flex items-center px-6 py-3 border border-transparent text-base font-medium rounded-md shadow-sm text-white bg-indigo-600 hover:bg-indigo-700 focus:outline-none focus:ring-2 focus:ring-offset-2 focus:ring-indigo-500 disabled:opacity-50 disabled:cursor-not-allowed transition-opacity"
              >
                {buttonText}
                {status === 'authenticated' && (
                  <ArrowRight className="ml-3 -mr-1 h-5 w-5" aria-hidden="true" />
                )}
              </button>
            </div>
          </div>
          {/* --- End Navigation Buttons Container --- */}

        </main>

        <Footer />
      </div>
    </>
  );
}
